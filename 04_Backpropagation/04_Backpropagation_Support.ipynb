{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) What task do you want to solve?\n",
    "    - You could use the MNIST dataset (as used in the previous practicals\n",
    "2) How does the network structure looks like?\n",
    "    - Build your own neural network\n",
    "        - Keep the code structure as simple as possible and\n",
    "        - use the same style as PyTorch, e. g. initializing a linear layer by Linear(in_features, out_features)\n",
    "            - use appropriate initialized weights for the linear layer weight matrices\n",
    "        - for this task you can stick by Linear layers only, to not make your code to complex\n",
    "3) Choose an appropriate loss function for your task (e. g. cross entropy loss for classification tasks)\n",
    "4) Perform backpropagation with your network network by using the SGD optimizer (you dont have to use batch sizes greater than 1, but you can :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_mnist_data(root_path='./data', batch_size=4):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "##################################\n",
    "# For matrices or arbitrary size #\n",
    "##################################\n",
    "class MyWeightTensor:\n",
    "    def __init__(self, shape: Tuple or int, init_weight_fn: Callable = np.random.randn, init_weights: 'MyWeightTensor' or np.ndarray or int or float = None):\n",
    "        assert isinstance(shape, tuple) or isinstance(shape, int) or isinstance(shape, float), f'Allowed shapes: tuple, int, float, got: {type(shape)}'\n",
    "        self.shape = shape\n",
    "\n",
    "        if init_weights is not None:\n",
    "            if isinstance(init_weights, MyWeightTensor):\n",
    "                self.values = init_weights.values\n",
    "            else:\n",
    "                if isinstance(shape, tuple):\n",
    "                    assert isinstance(init_weights, np.ndarray)\n",
    "                else:\n",
    "                    assert isinstance(init_weights, int) or isinstance(init_weights, float)\n",
    "                \n",
    "                self.values = init_weights\n",
    "        else:\n",
    "            if isinstance(shape, int):\n",
    "                self.shape = (self.shape,)\n",
    "                self.values = init_weight_fn(shape)\n",
    "            else:\n",
    "                self.values = init_weight_fn(*shape)\n",
    "    \n",
    "    @property\n",
    "    def T(self) -> 'MyWeightTensor':\n",
    "        _T = self.values.T\n",
    "        return MyWeightTensor(shape=_T.shape, init_weights=_T)\n",
    "    \n",
    "    def __add__(self, other) -> 'MyWeightTensor':\n",
    "        if isinstance(other, MyWeightTensor):\n",
    "            other = other.values\n",
    "        else:\n",
    "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
    "        \n",
    "        return MyWeightTensor(shape=self.values.shape, init_weights=self.values + other)\n",
    "\n",
    "    def __mul__(self, other) -> 'MyWeightTensor':\n",
    "        if isinstance(other, MyWeightTensor):\n",
    "            other = other.values\n",
    "        else:\n",
    "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
    "        \n",
    "        _dot = np.dot(self.values, other)\n",
    "\n",
    "        return MyWeightTensor(shape=_dot.shape, init_weights=_dot)\n",
    "\n",
    "\n",
    "###############################\n",
    "# For creating a linear layer #\n",
    "###############################\n",
    "class MyLinearLayer:\n",
    "    def __init__(self, in_features: int, out_features: int, init_weight_fn: Callable = np.random.randn) -> None:\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = MyWeightTensor(shape=(out_features, in_features), init_weight_fn=init_weight_fn)\n",
    "        self.bias = MyWeightTensor(shape=out_features, init_weight_fn=init_weight_fn)\n",
    "\n",
    "        self.latest_input = None\n",
    "        self.latest_output = None\n",
    "\n",
    "    def __call__(self, tensor: np.ndarray or MyWeightTensor) -> MyWeightTensor:\n",
    "        self.latest_input = tensor\n",
    "\n",
    "        bs = -1\n",
    "        if len(tensor.shape) == 2:\n",
    "            # batch size included\n",
    "            bs = tensor.shape[0]\n",
    "            _w = self.weights * tensor.T\n",
    "        else:\n",
    "            _w = self.weights * tensor\n",
    "        \n",
    "        _bias = self.bias.values\n",
    "        if bs != -1:\n",
    "            _bias = np.tile(_bias, bs).reshape(bs, -1)\n",
    "        \n",
    "        self.latest_output = (_w + _bias.T).T\n",
    "\n",
    "        return MyWeightTensor(shape=self.latest_output.shape, init_weights=self.latest_output)\n",
    "    \n",
    "    def derivative(self) -> float:\n",
    "        assert self.latest_output is not None, 'Cannot calculate grad without a single forward pass.'\n",
    "        # Linear activation derivation\n",
    "        return np.ones(shape=self.latest_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Creating a custom neural network #\n",
    "####################################\n",
    "\n",
    "def xavier_normal_init(*shape) -> np.ndarray:\n",
    "    assert len(shape) <= 2, 'Can only init max 2d tensors'\n",
    "    fan_in = shape[0]\n",
    "    if len(shape) == 1:\n",
    "        fan_out = fan_in\n",
    "    else:\n",
    "        fan_out = shape[1]\n",
    "    gain = 1.0\n",
    "\n",
    "    std = gain * np.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return np.random.normal(loc=0.0, scale=std, size=shape)\n",
    "\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self) -> None:\n",
    "        # init_weight_fn = lambda *shape: np.random.randn(*shape) / 10\n",
    "        init_weight_fn = lambda *shape: xavier_normal_init(*shape)\n",
    "        self.layers = [\n",
    "            MyLinearLayer(in_features=784, out_features=32, init_weight_fn=init_weight_fn),\n",
    "            MyLinearLayer(in_features=32, out_features=32, init_weight_fn=init_weight_fn),\n",
    "            MyLinearLayer(in_features=32, out_features=10, init_weight_fn=init_weight_fn)\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, tensor: np.ndarray) -> Any:\n",
    "        x = tensor\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def softmax(input: np.ndarray) -> np.ndarray:\n",
    "    _softmax = np.asarray([np.exp(_in) /np.sum(np.exp(_in), axis=0) for _in in input])\n",
    "\n",
    "    return _softmax\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, predictions: MyWeightTensor or np.ndarray, targets: MyWeightTensor or np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes cross entropy between targets and predictions.    \n",
    "        Returns: List of cross entropy losses (batch-wise)\n",
    "        \"\"\"\n",
    "        if isinstance(predictions, MyWeightTensor):\n",
    "            predictions = predictions.values\n",
    "        \n",
    "        if isinstance(targets, MyWeightTensor):\n",
    "            targets = targets.values\n",
    "\n",
    "        assert predictions.shape[0] == targets.shape[0]\n",
    "        if len(targets.shape) == 2:\n",
    "            targets = targets.reshape(-1)\n",
    "        predictions = torch.as_tensor(predictions)\n",
    "        targets = torch.as_tensor(targets)\n",
    "\n",
    "        loss = np.array([F.cross_entropy(pred, t).item() for pred, t in zip(predictions, targets)])\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def derivative(self) -> Callable:\n",
    "        # y_hat is the prediction\n",
    "        # y is the target value\n",
    "        def _derivative(y_hat: MyWeightTensor or np.ndarray, y: MyWeightTensor or np.ndarray) -> np.ndarray:\n",
    "            if isinstance(y_hat, MyWeightTensor):\n",
    "                y_hat = y_hat.values\n",
    "            \n",
    "            if isinstance(y, MyWeightTensor):\n",
    "                y = y.values\n",
    "\n",
    "            _y = np.zeros(shape=y_hat.shape)\n",
    "            np.put_along_axis(_y, y, 1, axis=-1)\n",
    "\n",
    "            y_hat = softmax(y_hat)\n",
    "\n",
    "            return y_hat - _y\n",
    "        \n",
    "        return _derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: MyNeuralNetwork, batch_size: int, learning_rate: float, loss_fn: Callable, epochs: int = 10):\n",
    "    train_loader, _ = load_mnist_data(batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = []\n",
    "        for imgs, targets in tqdm.tqdm(train_loader, desc=f'Training iteration {epoch + 1}'):\n",
    "\n",
    "            # for custom model\n",
    "            imgs = imgs.numpy()\n",
    "            targets = targets.numpy()\n",
    "\n",
    "            if len(targets.shape) == 1:\n",
    "                targets = targets.reshape(-1, 1)\n",
    "\n",
    "            imgs = imgs.reshape(-1, 28 * 28)\n",
    "\n",
    "            imgs = MyWeightTensor(shape=imgs.shape, init_weights=imgs)\n",
    "\n",
    "            outputs = model(imgs).values\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            avg_loss = np.mean(loss)\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += avg_loss\n",
    "\n",
    "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
    "            max_outputs = np.argmax(outputs, axis=1)\n",
    "            accuracy = (max_outputs == targets.flatten()).mean()\n",
    "            running_accuracy.append(accuracy)\n",
    "\n",
    "            #########################\n",
    "            # Start backpropagation #\n",
    "            #########################\n",
    "        \n",
    "\n",
    "            # Your code for backpropagation!\n",
    " \n",
    "            for layer in model.layers[::-1]:\n",
    "                grad_loss = loss_fn.derivative()(outputs, targets)\n",
    "                \n",
    "                grad_w = np.dot(grad_loss.T, layer.latest_input.values)\n",
    "                \n",
    "                grad_b = np.sum(grad_loss, axis=0)\n",
    "                \n",
    "                #print(learning_rate*grad_w)\n",
    "                updated_grad_w = (learning_rate * grad_w)\n",
    "                updated_grad_b = (learning_rate * grad_b)\n",
    "\n",
    "                print(\"shape of layer.weights.value: \")\n",
    "                print(layer.weights.values.shape)\n",
    "                print('upgraded shape :')\n",
    "                print(updated_grad_w.shape)\n",
    "\n",
    "                # Update weights and biases\n",
    "                layer.weights.values -= learning_rate * updated_grad_w\n",
    "                layer.bias.values -= learning_rate * updated_grad_b\n",
    "\n",
    "            #######################\n",
    "            # End backpropagation #\n",
    "            #######################\n",
    "                \n",
    "            \n",
    "\n",
    "        print(f'Epoch {epoch + 1} finished with loss: {running_loss / len(train_loader):.3f} and accuracy: {torch.tensor(running_accuracy).mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   0%|          | 0/15000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of layer.weights.value: \n",
      "(10, 32)\n",
      "upgraded shape :\n",
      "(10, 32)\n",
      "shape of layer.weights.value: \n",
      "(32, 32)\n",
      "upgraded shape :\n",
      "(10, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   0%|          | 0/15000 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,32) (10,32) (32,32) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rafid\\OneDrive\\Desktop\\Academic\\THI WS23\\Algorithm AI 2\\Algorithm AI 2 Exercises\\04_Backpropagation\\04_Backpropagation_Support.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mloss_fn\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\rafid\\OneDrive\\Desktop\\Academic\\THI WS23\\Algorithm AI 2\\Algorithm AI 2 Exercises\\04_Backpropagation\\04_Backpropagation_Support.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mprint\u001b[39m(updated_grad_w\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# Update weights and biases\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     layer\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mvalues \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m updated_grad_w\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     layer\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mvalues \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m updated_grad_b\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m#######################\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# End backpropagation #\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rafid/OneDrive/Desktop/Academic/THI%20WS23/Algorithm%20AI%202/Algorithm%20AI%202%20Exercises/04_Backpropagation/04_Backpropagation_Support.ipynb#X13sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m#######################\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,32) (10,32) (32,32) "
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Execute the training loop #\n",
    "#############################\n",
    "model = MyNeuralNetwork()\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('aai_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25301cabe4c6f833fd20f15b1b22933971919908771eb627a83fe325b4fb6671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
