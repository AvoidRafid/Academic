{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Today is about implementing the backpropagation algorithm.\n",
    "We will use the Stochastic Gradient Descent optimizer for optimizing the weights of a custom neural network.\n",
    "\n",
    "You can use numpy or torch for creating tensors, but not for the backpropagation (e.g. loss.backward() )!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have to consider the following steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) What task do you want to solve?\n",
    "    - You could use the MNIST dataset (as used in the previous practicals\n",
    "2) How does the network structure looks like?\n",
    "    - Build your own neural network\n",
    "        - Keep the code structure as simple as possible and\n",
    "        - use the same style as PyTorch, e. g. initializing a linear layer by Linear(in_features, out_features)\n",
    "            - use appropriate initialized weights for the linear layer weight matrices\n",
    "        - for this task you can stick by Linear layers only, to not make your code to complex\n",
    "3) Choose an appropriate loss function for your task (e. g. cross entropy loss for classification tasks)\n",
    "4) Perform backpropagation with your network network by using the SGD optimizer (you dont have to use batch sizes greater than 1, but you can :) )\n",
    "\n",
    "\n",
    "=> Code examples are provided for tasks 1, 2, 3 in an attached ipython notebook\n",
    "=> Only use them if you really need them, try to implement it by yourself first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to start now?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before you start, draw a simple neural network on a paper and do the backpropagation algorithm.\n",
    "E. g. using 3 layers with 2 neurons each and calculate the update of a weight in the very first layer.\n",
    "Use a simple loss function (which also has a simple to derivative)\n",
    "\n",
    "After the calculations, try to implement the backpropagation algorithm with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "# Consider the following steps:\n",
    "# 1) Loop through your training data\n",
    "#   1. 1) Choose number of epochs (How often do you want to loop through your complete dataset?)\n",
    "# 2) Forward the data through your network\n",
    "# 3) Calculate the loss\n",
    "# 4) Perform backpropagation with SGD and update the weights\n",
    "#   4. 1) Choose a learning rate to update your weights\n",
    "# Repeat 1, 2, 3, 4 until the training converges or maximum epochs are reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input (X) = [x1, x2]\\nTarget (y) = [1]\\n\\n--- Forward Pass ---\\n\\n1. Input Layer to Hidden Layer:\\n   w1 = weight from x1 to h1\\n   w2 = weight from x2 to h1\\n   w3 = weight from x1 to h2\\n   w4 = weight from x2 to h2\\n   b1 = bias for h1\\n   b2 = bias for h2\\n\\n   h1 = sigmoid(w1*x1 + w2*x2 + b1)\\n   h2 = sigmoid(w3*x1 + w4*x2 + b2)\\n\\n2. Hidden Layer to Output Layer:\\n   w5 = weight from h1 to output\\n   w6 = weight from h2 to output\\n   b3 = bias for output\\n\\n   output = sigmoid(w5*h1 + w6*h2 + b3)\\n\\n3. Calculate MSE Loss:\\n   loss = 0.5 * (output - y)^2\\n\\n--- Backward Pass ---\\n\\n1. Compute Gradients:\\n   - d_loss/d_output = output - y\\n   - d_output/d_w5 = h1 * output * (1 - output)\\n   - d_output/d_w6 = h2 * output * (1 - output)\\n   - d_output/d_h1 = w5 * output * (1 - output)\\n   - d_output/d_h2 = w6 * output * (1 - output)\\n   - d_h1/d_w1 = x1 * h1 * (1 - h1)\\n   - d_h1/d_w2 = x2 * h1 * (1 - h1)\\n   - d_h2/d_w3 = x1 * h2 * (1 - h2)\\n   - d_h2/d_w4 = x2 * h2 * (1 - h2)\\n\\n2. Backpropagate Gradients:\\n   - Update weights using the learning rate:\\n     w1 = w1 - learning_rate * d_loss/d_output * d_output/d_h1 * d_h1/d_w1\\n   - Similarly, update other weights and biases.\\n\\n3. Repeat the process for multiple epochs until convergence.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Input (X) = [x1, x2]\n",
    "Target (y) = [1]\n",
    "\n",
    "--- Forward Pass ---\n",
    "\n",
    "1. Input Layer to Hidden Layer:\n",
    "   w1 = weight from x1 to h1\n",
    "   w2 = weight from x2 to h1\n",
    "   w3 = weight from x1 to h2\n",
    "   w4 = weight from x2 to h2\n",
    "   b1 = bias for h1\n",
    "   b2 = bias for h2\n",
    "\n",
    "   h1 = sigmoid(w1*x1 + w2*x2 + b1)\n",
    "   h2 = sigmoid(w3*x1 + w4*x2 + b2)\n",
    "\n",
    "2. Hidden Layer to Output Layer:\n",
    "   w5 = weight from h1 to output\n",
    "   w6 = weight from h2 to output\n",
    "   b3 = bias for output\n",
    "\n",
    "   output = sigmoid(w5*h1 + w6*h2 + b3)\n",
    "\n",
    "3. Calculate MSE Loss:\n",
    "   loss = 0.5 * (output - y)^2\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "--- Backward Pass ---\n",
    "\n",
    "1. Compute Gradients:\n",
    "   - d_loss/d_output = output - y\n",
    "   - d_output/d_w5 = h1 * output * (1 - output)\n",
    "   - d_output/d_w6 = h2 * output * (1 - output)\n",
    "   - d_output/d_h1 = w5 * output * (1 - output)\n",
    "   - d_output/d_h2 = w6 * output * (1 - output)\n",
    "   - d_h1/d_w1 = x1 * h1 * (1 - h1)\n",
    "   - d_h1/d_w2 = x2 * h1 * (1 - h1)\n",
    "   - d_h2/d_w3 = x1 * h2 * (1 - h2)\n",
    "   - d_h2/d_w4 = x2 * h2 * (1 - h2)\n",
    "\n",
    "2. Backpropagate Gradients:\n",
    "   - Update weights using the learning rate:\n",
    "     w1 = w1 - learning_rate * d_loss/d_output * d_output/d_h1 * d_h1/d_w1\n",
    "   - Similarly, updating other weights and biases.\n",
    "\n",
    "3. Repeating the process for multiple epochs until convergence.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.024850926922809005\n",
      "Epoch [20/100], Loss: 0.019815846923920258\n",
      "Epoch [30/100], Loss: 0.015923917754564688\n",
      "Epoch [40/100], Loss: 0.012899201199448299\n",
      "Epoch [50/100], Loss: 0.010531406234111765\n",
      "Epoch [60/100], Loss: 0.008662813808854386\n",
      "Epoch [70/100], Loss: 0.007175820212663337\n",
      "Epoch [80/100], Loss: 0.00598270229969578\n",
      "Epoch [90/100], Loss: 0.005017757799181896\n",
      "Epoch [100/100], Loss: 0.0042314681042219015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "input_size = 2\n",
    "hidden_layer_size = 2\n",
    "output_size = 1\n",
    "\n",
    "np.random.seed(42)\n",
    "weights_input_hidden = np.random.rand(input_size, hidden_layer_size)  # shape 2x2\n",
    "weights_hidden_output = np.random.rand(hidden_layer_size, output_size) #shape 2x1\n",
    "\n",
    "bias_hidden = np.zeros((1, hidden_layer_size))\n",
    "bias_output = np.zeros((1,output_size))\n",
    "\n",
    "#Training\n",
    "X = np.array([[0.5,0.6]])\n",
    "y = np.array([[0.8]])\n",
    "\n",
    "\n",
    "#Parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass\n",
    "    # Input to Hidden Layer\n",
    "\n",
    "    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "    # Hidden Layer to Output Layer\n",
    "    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "    predicted_output = sigmoid(output_input)\n",
    "\n",
    "    # Calculating Loss\n",
    "    loss = 0.5 * np.sum((predicted_output - y) ** 2)\n",
    "\n",
    "    # Backward Pass\n",
    "    # Computing Gradients\n",
    "    d_loss_d_output = predicted_output - y\n",
    "    d_output_d_hidden = sigmoid_derivative(predicted_output)\n",
    "    d_hidden_d_input = sigmoid_derivative(hidden_output)\n",
    "\n",
    "    # Updating weights and biases using the chain rule\n",
    "    weights_hidden_output -= learning_rate * np.dot(hidden_output.T, d_loss_d_output * d_output_d_hidden)\n",
    "    weights_input_hidden -= learning_rate * np.dot(X.T, np.dot(d_loss_d_output * d_output_d_hidden, weights_hidden_output.T) * d_hidden_d_input)\n",
    "\n",
    "    bias_output -= learning_rate * np.sum(d_loss_d_output * d_output_d_hidden, axis=0, keepdims=True)\n",
    "    bias_hidden -= learning_rate * np.sum(np.dot(d_loss_d_output * d_output_d_hidden, weights_hidden_output.T) * d_hidden_d_input, axis=0, keepdims=True)\n",
    "\n",
    "    # Printing the loss for every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('aai_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25301cabe4c6f833fd20f15b1b22933971919908771eb627a83fe325b4fb6671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
