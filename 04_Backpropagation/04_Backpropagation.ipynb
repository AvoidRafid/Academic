{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Today is about implementing the backpropagation algorithm.\n",
    "We will use the Stochastic Gradient Descent optimizer for optimizing the weights of a custom neural network.\n",
    "\n",
    "You can use numpy or torch for creating tensors, but not for the backpropagation (e.g. loss.backward() )!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have to consider the following steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) What task do you want to solve?\n",
    "    - You could use the MNIST dataset (as used in the previous practicals\n",
    "2) How does the network structure looks like?\n",
    "    - Build your own neural network\n",
    "        - Keep the code structure as simple as possible and\n",
    "        - use the same style as PyTorch, e. g. initializing a linear layer by Linear(in_features, out_features)\n",
    "            - use appropriate initialized weights for the linear layer weight matrices\n",
    "        - for this task you can stick by Linear layers only, to not make your code to complex\n",
    "3) Choose an appropriate loss function for your task (e. g. cross entropy loss for classification tasks)\n",
    "4) Perform backpropagation with your network network by using the SGD optimizer (you dont have to use batch sizes greater than 1, but you can :) )\n",
    "\n",
    "\n",
    "=> Code examples are provided for tasks 1, 2, 3 in an attached ipython notebook\n",
    "=> Only use them if you really need them, try to implement it by yourself first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to start now?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before you start, draw a simple neural network on a paper and do the backpropagation algorithm.\n",
    "E. g. using 3 layers with 2 neurons each and calculate the update of a weight in the very first layer.\n",
    "Use a simple loss function (which also has a simple to derivative)\n",
    "\n",
    "After the calculations, try to implement the backpropagation algorithm with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "def load_mnist_data(root_path='./data', batch_size=4):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "class MyWeightTensor:\n",
    "    def __init__(self, shape: Tuple or int, init_weight_fn: Callable = np.random.randn, init_weights: 'MyWeightTensor' or np.ndarray or int or float = None):\n",
    "        assert isinstance(shape, tuple) or isinstance(shape, int) or isinstance(shape, float), f'Allowed shapes: tuple, int, float, got: {type(shape)}'\n",
    "        self.shape = shape\n",
    "\n",
    "        if init_weights is not None:\n",
    "            if isinstance(init_weights, MyWeightTensor):\n",
    "                self.values = init_weights.values\n",
    "            else:\n",
    "                if isinstance(shape, tuple):\n",
    "                    assert isinstance(init_weights, np.ndarray)\n",
    "                else:\n",
    "                    assert isinstance(init_weights, int) or isinstance(init_weights, float)\n",
    "                \n",
    "                self.values = init_weights\n",
    "        else:\n",
    "            if isinstance(shape, int):\n",
    "                self.shape = (self.shape,)\n",
    "                self.values = init_weight_fn(shape)\n",
    "            else:\n",
    "                self.values = init_weight_fn(*shape)\n",
    "    \n",
    "    @property\n",
    "    def T(self) -> 'MyWeightTensor':\n",
    "        _T = self.values.T\n",
    "        return MyWeightTensor(shape=_T.shape, init_weights=_T)\n",
    "    \n",
    "    def __add__(self, other) -> 'MyWeightTensor':\n",
    "        if isinstance(other, MyWeightTensor):\n",
    "            other = other.values\n",
    "        else:\n",
    "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
    "        \n",
    "        return MyWeightTensor(shape=self.values.shape, init_weights=self.values + other)\n",
    "\n",
    "    def __mul__(self, other) -> 'MyWeightTensor':\n",
    "        if isinstance(other, MyWeightTensor):\n",
    "            other = other.values\n",
    "        else:\n",
    "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
    "        \n",
    "        _dot = np.dot(self.values, other)\n",
    "\n",
    "        return MyWeightTensor(shape=_dot.shape, init_weights=_dot)\n",
    "\n",
    "\n",
    "###############################\n",
    "# For creating a linear layer #\n",
    "###############################\n",
    "class MyLinearLayer:\n",
    "    def __init__(self, in_features: int, out_features: int, init_weight_fn: Callable = np.random.randn) -> None:\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = MyWeightTensor(shape=(out_features, in_features), init_weight_fn=init_weight_fn)\n",
    "        self.bias = MyWeightTensor(shape=out_features, init_weight_fn=init_weight_fn)\n",
    "\n",
    "        self.latest_input = None\n",
    "        self.latest_output = None\n",
    "\n",
    "    def __call__(self, tensor: np.ndarray or MyWeightTensor) -> MyWeightTensor:\n",
    "        self.latest_input = tensor\n",
    "\n",
    "        bs = -1\n",
    "        if len(tensor.shape) == 2:\n",
    "            # batch size included\n",
    "            bs = tensor.shape[0]\n",
    "            _w = self.weights * tensor.T\n",
    "        else:\n",
    "            _w = self.weights * tensor\n",
    "        \n",
    "        _bias = self.bias.values\n",
    "        if bs != -1:\n",
    "            _bias = np.tile(_bias, bs).reshape(bs, -1)\n",
    "        \n",
    "        self.latest_output = (_w + _bias.T).T\n",
    "\n",
    "        return MyWeightTensor(shape=self.latest_output.shape, init_weights=self.latest_output)\n",
    "    \n",
    "    def derivative(self) -> float:\n",
    "        assert self.latest_output is not None, 'Cannot calculate grad without a single forward pass.'\n",
    "        # Linear activation derivation\n",
    "        return np.ones(shape=self.latest_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your loss function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "# Consider the following steps:\n",
    "# 1) Loop through your training data\n",
    "#   1. 1) Choose number of epochs (How often do you want to loop through your complete dataset?)\n",
    "# 2) Forward the data through your network\n",
    "# 3) Calculate the loss\n",
    "# 4) Perform backpropagation with SGD and update the weights\n",
    "#   4. 1) Choose a learning rate to update your weights\n",
    "# Repeat 1, 2, 3, 4 until the training converges or maximum epochs are reached"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('aai_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25301cabe4c6f833fd20f15b1b22933971919908771eb627a83fe325b4fb6671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
